{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a13b139",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 40\n",
    "max_seq_len = 1024 #irrelevant\n",
    "num_heads = 5\n",
    "drop = 0.1\n",
    "vocab = 16000\n",
    "num_blocks = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52797c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "pos = torch.arange(0, 5, dtype=torch.float).unsqueeze(1)\n",
    "div_term = torch.exp(torch.arange(0, 4, 2).float() * -(math.log(10000.0)/4))\n",
    "\n",
    "result = pos * div_term\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83b9613e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(torch.nn.Module):\n",
    "    def __init__(self, max_seq_len, emb_dim):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_seq_len, emb_dim)\n",
    "        pos = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1) #unsqueeze makes it [s, 1] instead of [s]\n",
    "        div_term = torch.exp(torch.arange(0, emb_dim, 2).float() * -(math.log(10000.0)/emb_dim))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(pos*div_term)\n",
    "        pe[:, 1::2] = torch.cos(pos*div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.pe[:, :x.size(1)] #[B, x's S, E]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22b1f076",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, emb_dim, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        self.qkv_proj = torch.nn.Linear(emb_dim, emb_dim * 3)\n",
    "        self.wo_proj = torch.nn.Linear(emb_dim, emb_dim)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, S, E = x.shape\n",
    "        EMBD_HEAD = int(emb_dim / num_heads)\n",
    "\n",
    "        qry, key, val = self.qkv_proj(x).split(emb_dim, dim=-1) #at this point shapes (B, S, E) = \n",
    "        #(B,S,E,embedhead*num_heads)\n",
    "        #e.g. if E is 30 and num_heads = 5 then emb_head = 6\n",
    "        qry = qry.reshape(B, S, num_heads, EMBD_HEAD).transpose(1, 2) #split into (B, S, num_heads, emb_head)\n",
    "        #after transpose, B batches, num_heads per batch, S x emb_head matrices\n",
    "        key = key.reshape(B, S, num_heads, EMBD_HEAD).transpose(1, 2)\n",
    "        val = val.reshape(B, S, num_heads, EMBD_HEAD).transpose(1, 2)\n",
    "\n",
    "       \n",
    "        att = qry @ key.transpose(-1, -2) / torch.sqrt(torch.tensor(EMBD_HEAD))\n",
    "        # qry = S x embhead, key = embhead x S and scale with the size of each head\n",
    "        \n",
    "        att = torch.nn.functional.softmax(att, dim=-1) #softmax along rows...(B, num_heads, S, S)\n",
    "        out = (att @ val).transpose(1, 2).reshape(B, S, E) #(B, numheads, S, embhead) to #(B, S, numheads, embhead)\n",
    "        #to (B,S,E) so per batch, S numhead x embhead matrices, one per sequence word to one S x E matrix\n",
    "        return self.wo_proj(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d374ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(torch.nn.Module):\n",
    "    def __init__(self, emb_dim, drop):\n",
    "        super().__init__()\n",
    "        self.c_fc   = torch.nn.Linear(emb_dim, emb_dim * 4) #By using a higher-dimensional space (e.g., *4),\n",
    "        #the model can potentially learn more intricate and non-linear relationships within the data\n",
    "        self.relu   = torch.nn.ReLU()\n",
    "        self.c_proj = torch.nn.Linear(emb_dim * 4, emb_dim)\n",
    "        self.drop   = torch.nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a1396f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(torch.nn.Module):\n",
    "    def __init__(self, emb_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.ln_1 = torch.nn.LayerNorm(emb_dim)\n",
    "        self.attn = MultiHeadAttention(emb_dim, num_heads)\n",
    "        self.ln_2 = torch.nn.LayerNorm(emb_dim)\n",
    "        self.ffww = FeedForward(emb_dim, drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ln_1(x + self.attn(x))\n",
    "        x = self.ln_2(x + self.ffww(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb9e0a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab, emb_dim, drop, num_blocks, num_heads, max_seq_len):\n",
    "        super().__init__()\n",
    "        self.tok_emb = torch.nn.Embedding(vocab, emb_dim)\n",
    "        self.drop    = torch.nn.Dropout(drop)\n",
    "        self.blocks  = torch.nn.ModuleList([AttentionBlock(emb_dim, num_heads) for _ in range(num_blocks)])\n",
    "        self.pos = PositionalEncoding(max_seq_len, emb_dim)\n",
    "        self.vocab.weights = self.tok_emb.weights \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        this_seq_len = min(x.size(1), max_seq_len)\n",
    "        # Slice the input tensor to the desired sequence length\n",
    "        x = x[:, :this_seq_len]\n",
    "        tok_emb = self.tok_emb(x)\n",
    "        pos_emb = self.pos(x)\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.drop(x)\n",
    "        for block in self.blocks: x = block(x)\n",
    "        return x #output (B, S, E)\n",
    "\n",
    "    def num_params(self):\n",
    "        gpt_params = sum(p.numel() for p in self.parameters()) #no. of parameters\n",
    "        emb_params = self.tok_emb.weight.numel() #no of parameters (weights) in the token embeddings\n",
    "        print(f\"Total Parameters: {gpt_params} | Embedding: {emb_params}\")\n",
    "        return { 'gpt_params': gpt_params, 'emb_params': emb_params }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764173bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
